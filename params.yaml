Data_upload:
  upload_from_local: 
    path: "C:\\Users\\User\\Desktop\\mlproject\\notebook\\data\\stud1.csv"
    a: "train1.csv"
    b: "test1.csv"
    c: "stud1.csv"
  upload_from_gcp:
    json_file_path: "C:\\Users\\User\\Desktop\\DQM\\Data_Quality\\credentials-python-storage.json"
    bucket_name: "tmrw_scraping_data"
    file_path_name: "pdp/ajio/2023/01/14/PDP_Ajio_2023_01_13_PDP_Batch 8_500_Products20230113.csv"
    cloud_name: "gcp"


base:
    random_state: 42
    test_split_ratio: 0.2

lasso_tuned:
  alpha : [0.1, 1, 5, 10, 20, 40, 60, 80, 100]

Ridge_tuned:
  alpha : [0.1, 1, 5]

K_Neighbors_Regressor_tuned:
  n_neighbors: [5,3]      # number of neighbors to consider (default is 5)
  weights: ['uniform','distance']  # weighting scheme for neighbors ('uniform' or 'distance')
  algorithm: ['auto']  # algorithm to use for computing nearest neighbors ('auto', 'ball_tree', 'kd_tree', or 'brute')  
  p: [1,2]                # distance metric to use (1 for manhattan distance, 2 for euclidean distance, etc.)
  metric: ['minkowski']  # distance metric to use (can also use other metrics like 'cosine', 'jaccard', etc.)

Decision_Tree_Tuned:
  max_depth: [2,10]
  min_samples_leaf: [10,50]
  min_samples_split: [10,20]

Random_Forest_Regressor_Tuned:
  n_estimators : [5,21,51,101] # number of trees in the random forest
  max_features : ['auto', 'sqrt'] # number of features in consideration at every split
  max_depth : [int(x) for x in np.linspace(10, 120, num = 12)] # maximum number of levels allowed in each decision tree
  min_samples_split : [2, 6, 10] # minimum sample number to split a node
  min_samples_leaf : [1, 3, 4] # minimum sample number that can be stored in a leaf node
  bootstrap : [True, False]


XGBRegressor_Tuned:
  n_estimators: [int(x) for x in np.linspace(10, 200, num = 20)]
  max_depth: [int(x) for x in np.linspace(2, 15, num = 14)]
  learning_rate: [0.0001,0.001,0.01,0.1,0.1,0.2,0.3]
  min_samples_split: [int(x) for x in np.linspace(20, 40, num = 20)]
  min_samples_leaf: [int(x) for x in np.linspace(10, 20, num = 10)]


CatBoosting_Regressor_Tuned :
  depth: [6,8,10]
  learning_rate: [0.01, 0.05, 0.1]
  iterations: [30, 50, 100]


AdaBoost_Regressor_Tuned: 
  n_estimators: [int(x) for x in np.linspace(10, 200, num = 20)]
  learning_rate: [0.0001,0.001,0.01,0.1,0.1,0.2,0.3]
  loss: ['linear','square','exponential']
  
  
  


